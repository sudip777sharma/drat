{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "\n",
    "# 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_page(title):\n",
    "    \"\"\"\n",
    "    Get the wikipedia page given a title\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return wikipedia.page(title)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return wikipedia.page(e.options[0])\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursively_find_all_pages(titles, titles_so_far=set(), max_pages=2):\n",
    "    \"\"\"\n",
    "    Recursively find up to a specified number of pages linked to the Wikipedia titles in the list\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    \n",
    "    titles = list(set(titles) - titles_so_far)\n",
    "\n",
    "    titles_so_far.update(titles)\n",
    "\n",
    "    for title in titles:\n",
    "        page = get_wiki_page(title)\n",
    "        if page is None:\n",
    "            continue\n",
    "        all_pages.append(page)\n",
    "\n",
    "        if len(all_pages) >= max_pages:\n",
    "            break\n",
    "\n",
    "        new_pages = recursively_find_all_pages(page.links, titles_so_far, max_pages=max_pages)\n",
    "        for pg in new_pages:\n",
    "            if pg.title not in [p.title for p in all_pages]:\n",
    "                all_pages.append(pg)\n",
    "                if len(all_pages) >= max_pages:\n",
    "                    break\n",
    "\n",
    "        titles_so_far.update(page.links)\n",
    "        \n",
    "        if len(all_pages) >= max_pages:\n",
    "            break\n",
    "        \n",
    "    return all_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Constitution of India', 'Code of Civil Procedure (India)', 'Code of Criminal Procedure (India)', 'Indian Evidence Act', 'Indian Penal Code']\n",
    "# topics = ['Code of Civil Procedure (India)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = recursively_find_all_pages(topics[0])\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Set\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_long(\n",
    "    long_text: str, long_text_tokens: bool = False, max_len: int = 120\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reduce a long text to a maximum of `max_len` tokens by potentially cutting at a sentence end\n",
    "    \"\"\"\n",
    "    if not long_text_tokens:\n",
    "        long_text_tokens = count_tokens(long_text)\n",
    "    if long_text_tokens > max_len:\n",
    "        sentences = sent_tokenize(long_text.replace(\"\\n\", \" \"))\n",
    "        ntokens = 0\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            ntokens += 1 + count_tokens(sentence)\n",
    "            if ntokens > max_len:\n",
    "                return \". \".join(sentences[:i]) + \".\"\n",
    "\n",
    "    return long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",\n",
    "    \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",\n",
    "    \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",\n",
    "    \"References and notes\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(\n",
    "    wiki_text: str,\n",
    "    title: str,\n",
    "    max_len: int = 1500,\n",
    "    discard_categories: Set[str] = discard_categories,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract the sections of a Wikipedia page, discarding the references and other low information sections\n",
    "    \"\"\"\n",
    "    if len(wiki_text) == 0:\n",
    "        return []\n",
    "\n",
    "    # find all headings and the coresponding contents\n",
    "    headings = re.findall(\"==+ .* ==+\", wiki_text)\n",
    "\n",
    "    for heading in headings:\n",
    "        wiki_text = wiki_text.replace(heading, \"==+ !! ==+\")\n",
    "\n",
    "    contents = wiki_text.split(\"==+ !! ==+\")\n",
    "\n",
    "    contents = [c.strip() for c in contents]\n",
    "    assert len(headings) == len(contents) - 1\n",
    "\n",
    "    cont = contents.pop(0).strip()\n",
    "    outputs = [(title, \"Summary\", cont, count_tokens(cont)+4)]\n",
    "\n",
    "    # discard the discard categories, accounting for a tree structure\n",
    "    max_level = 100\n",
    "    keep_group_level = max_level\n",
    "    remove_group_level = max_level\n",
    "    nheadings, ncontents = [], []\n",
    "    for heading, content in zip(headings, contents):\n",
    "        plain_heading = \" \".join(heading.split(\" \")[1:-1])\n",
    "        num_equals = len(heading.split(\" \")[0])\n",
    "        if num_equals <= keep_group_level:\n",
    "            keep_group_level = max_level\n",
    "\n",
    "        if num_equals > remove_group_level:\n",
    "            if (\n",
    "                num_equals <= keep_group_level\n",
    "            ):\n",
    "                continue\n",
    "        keep_group_level = max_level\n",
    "        if plain_heading in discard_categories:\n",
    "            remove_group_level = num_equals\n",
    "            keep_group_level = max_level\n",
    "            continue\n",
    "        nheadings.append(heading.replace(\"=\", \"\").strip())\n",
    "        ncontents.append(content)\n",
    "        remove_group_level = max_level\n",
    "\n",
    "    # count the tokens of each section\n",
    "    ncontent_ntokens = [\n",
    "        count_tokens(c)\n",
    "        + 3\n",
    "        + count_tokens(\" \".join(h.split(\" \")[1:-1]))\n",
    "        - (1 if len(c) == 0 else 0)\n",
    "        for h, c in zip(nheadings, ncontents)\n",
    "    ]\n",
    "\n",
    "    # Create a tuple of (title, section_name, content, number of tokens)\n",
    "    outputs += [(title, h, c, t) if t<max_len \n",
    "                else (title, h, reduce_long(c, max_len), count_tokens(reduce_long(c,max_len))) \n",
    "                    for h, c, t in zip(nheadings, ncontents, ncontent_ntokens)]\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for page in pages:\n",
    "    res += extract_sections(page.content, page.title)\n",
    "df = pd.DataFrame(res, columns=[\"title\", \"heading\", \"content\", \"tokens\"])\n",
    "df = df[df.tokens>40]\n",
    "df = df.drop_duplicates(['title','heading'])\n",
    "df = df.reset_index().drop('index',axis=1) # reset index\n",
    "df = df.iloc[:5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed_data/processed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetunegpt3eg_vevn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
